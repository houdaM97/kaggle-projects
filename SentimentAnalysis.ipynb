{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "338ed9bb5ce9429c99797447343b1bb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f7e93a129dc445deb87eb3d7d7ae59cc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c0a8478d74ec4d40840094afabe7f025",
              "IPY_MODEL_93676635e9b240c8932d1804aefb4f81"
            ]
          }
        },
        "f7e93a129dc445deb87eb3d7d7ae59cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0a8478d74ec4d40840094afabe7f025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a5d72a4cffb7492186f5399ad0a45fa1",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22b45f6f170844cd8407eca0d8e67370"
          }
        },
        "93676635e9b240c8932d1804aefb4f81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_88cc1712981e4465995b1e49edcde707",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 361/361 [00:00&lt;00:00, 8.98kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e78acb3dabae4a63909f586886207c3c"
          }
        },
        "a5d72a4cffb7492186f5399ad0a45fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22b45f6f170844cd8407eca0d8e67370": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88cc1712981e4465995b1e49edcde707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e78acb3dabae4a63909f586886207c3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "74e1d76cc0fc4063ae9763b018d6a7f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c2ca20470e73441582eaec0ffb8a93cd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f1fddb25162647bb8a5f44327c929c8b",
              "IPY_MODEL_b1999876252742518e89dae8171a839a"
            ]
          }
        },
        "c2ca20470e73441582eaec0ffb8a93cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1fddb25162647bb8a5f44327c929c8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_407217b4773b4d4e98c76e9ad45d240d",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7dd7eea7ec0401c88c19cb0c698189d"
          }
        },
        "b1999876252742518e89dae8171a839a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bde9a203edd74985aa926484a9f5986f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 232k/232k [00:00&lt;00:00, 415kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e860ac0d6ffa400bb434c18697ae8ff2"
          }
        },
        "407217b4773b4d4e98c76e9ad45d240d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7dd7eea7ec0401c88c19cb0c698189d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bde9a203edd74985aa926484a9f5986f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e860ac0d6ffa400bb434c18697ae8ff2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0ef34881802489f952b36fe65016be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_acae6beaa77440dab477b3c2173a7778",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8fc9d17c93a94a49a8480e4dc5a22fda",
              "IPY_MODEL_a78d42d45aff4ad09bddc700eca7b302"
            ]
          }
        },
        "acae6beaa77440dab477b3c2173a7778": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8fc9d17c93a94a49a8480e4dc5a22fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d117b05cd1004c1e89f9f0f057eead42",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 536063208,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 536063208,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b95304e13c62420a86bee7cea0c3645a"
          }
        },
        "a78d42d45aff4ad09bddc700eca7b302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9cdb3f2e0eb344a3963d37cf085a90ed",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 536M/536M [00:45&lt;00:00, 11.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8239035c63bf49fa9b4e3eed0955266a"
          }
        },
        "d117b05cd1004c1e89f9f0f057eead42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b95304e13c62420a86bee7cea0c3645a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9cdb3f2e0eb344a3963d37cf085a90ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8239035c63bf49fa9b4e3eed0955266a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/houdaM97/kaggle-projects/blob/master/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li7CcYRbofwx",
        "colab_type": "code",
        "outputId": "6d889f4b-76fc-403b-b820-6acba1051411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 45kB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.8)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (2.0.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.17.5)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.34.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (2.0.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.11.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.16.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (45.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow",
                  "tensorflow_core"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IhY_l7625F0",
        "colab_type": "code",
        "outputId": "2de3b40a-92b2-425e-d075-baf85364daa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJLbNxvDb6Cw",
        "colab_type": "code",
        "outputId": "f8117734-81f8-4e3c-8b04-ace91ef8d0f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\r\u001b[K     |▊                               | 10kB 22.3MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40kB 3.5MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51kB 2.2MB/s eta 0:00:01\r\u001b[K     |████▏                           | 61kB 2.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 81kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 481kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /tensorflow-2.1.0/python3.6 (from transformers) (1.18.1)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 43.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /tensorflow-2.1.0/python3.6 (from transformers) (2.22.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 38.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (1.25.8)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: six in /tensorflow-2.1.0/python3.6 (from sacremoses->transformers) (1.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.2)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.10 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.10)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.10->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=a8a557c9365d63b8841906af94f31436a222d401a75ce9ecbb65e4958e9dc679\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZkfrjkomDlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import TFBertForSequenceClassification\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsdN3G_ypLXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"train.tsv\", sep='\\t', engine='python')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAIoLBdrqAIO",
        "colab_type": "code",
        "outputId": "1e823662-92f5-4ec8-e567-18aaa22d5178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TaxHB1pqrw4",
        "colab_type": "code",
        "outputId": "33380028-db0b-45bd-cb4d-7957c581506e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "338ed9bb5ce9429c99797447343b1bb3",
            "f7e93a129dc445deb87eb3d7d7ae59cc",
            "c0a8478d74ec4d40840094afabe7f025",
            "93676635e9b240c8932d1804aefb4f81",
            "a5d72a4cffb7492186f5399ad0a45fa1",
            "22b45f6f170844cd8407eca0d8e67370",
            "88cc1712981e4465995b1e49edcde707",
            "e78acb3dabae4a63909f586886207c3c",
            "74e1d76cc0fc4063ae9763b018d6a7f2",
            "c2ca20470e73441582eaec0ffb8a93cd",
            "f1fddb25162647bb8a5f44327c929c8b",
            "b1999876252742518e89dae8171a839a",
            "407217b4773b4d4e98c76e9ad45d240d",
            "d7dd7eea7ec0401c88c19cb0c698189d",
            "bde9a203edd74985aa926484a9f5986f",
            "e860ac0d6ffa400bb434c18697ae8ff2",
            "e0ef34881802489f952b36fe65016be2",
            "acae6beaa77440dab477b3c2173a7778",
            "8fc9d17c93a94a49a8480e4dc5a22fda",
            "a78d42d45aff4ad09bddc700eca7b302",
            "d117b05cd1004c1e89f9f0f057eead42",
            "b95304e13c62420a86bee7cea0c3645a",
            "9cdb3f2e0eb344a3963d37cf085a90ed",
            "8239035c63bf49fa9b4e3eed0955266a"
          ]
        }
      },
      "source": [
        "config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=5)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "338ed9bb5ce9429c99797447343b1bb3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74e1d76cc0fc4063ae9763b018d6a7f2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0ef34881802489f952b36fe65016be2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=536063208, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgi9vpiwuRO-",
        "colab_type": "code",
        "outputId": "9144d030-8e15-404e-d124-06adcdc524d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "sb.countplot(x='Sentiment', data = df, palette='RdBu')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb695a4e358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEGCAYAAACkQqisAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSJI6M0QkSZ0ZIpKkzgwRSVJn/x/mt4xU3hYUzQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBd1SvnZsFOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_seq_length = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdLs9YqJqtMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_bool_to_int(x):\n",
        "  if x == False :\n",
        "    return 1\n",
        "  else :\n",
        "    return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDr6eUh-7Xjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(example):\n",
        "  inputs = tokenizer.encode_plus(example, max_length=max_seq_length, pad_to_max_length=True, add_special_tokens=True)\n",
        "  iids = inputs['input_ids']\n",
        "  \n",
        "  \"\"\"if len(iids) > (max_seq_length - 2) :\n",
        "    iids = iids[0:(max_seq_length - 2)]\n",
        "    token_type_ids = inputs['token_type_ids'][0:(max_seq_length - 2)]\n",
        "  \n",
        "  if len(iids) < (max_seq_length - 2) :\n",
        "    iids.append(0)\n",
        "    token_type_ids.append(0)\"\"\"\n",
        "\n",
        "  input_ids = tf.constant([iids])\n",
        "  attention_mask = tf.constant([inputs['attention_mask']])\n",
        "  token_type_ids = tf.constant([inputs['token_type_ids']])\n",
        "  #input_mask = (input_ids == 0)\n",
        "  #input_mask = tf.map_fn(convert_bool_to_int, input_mask[0], dtype = tf.int32)\n",
        "  #segment_tensors = tf.constant([token_type_ids])\n",
        "  #return {'input_ids': input_ids, 'input_mask': input_mask, 'segment_tensors': segment_tensors}\n",
        "  return {'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gjgJTj2e0Zz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Features'] = df['Phrase'].apply(convert_examples_to_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4702wovfOoF",
        "colab_type": "code",
        "outputId": "9d6ac30c-372a-418b-992d-16daf160f06c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df['Features'].to_numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([{'input_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[  101,  1037,  2186,  1997,  9686, 17695, 18673, 14313,  1996,\n",
              "        15262,  3351,  2008,  2054,  2003,  2204,  2005,  1996, 13020,\n",
              "         2003,  2036,  2204,  2005,  1996, 25957,  4063,  1010,  2070,\n",
              "         1997,  2029,  5681,  2572, 25581,  2021,  3904,  1997,  2029,\n",
              "         8310,  2000,  2172,  1997,  1037,  2466,  1012,   102,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int32)>},\n",
              "       {'input_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[  101,  1037,  2186,  1997,  9686, 17695, 18673, 14313,  1996,\n",
              "        15262,  3351,  2008,  2054,  2003,  2204,  2005,  1996, 13020,\n",
              "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int32)>},\n",
              "       {'input_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[ 101, 1037, 2186,  102,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int32)>},\n",
              "       ...,\n",
              "       {'input_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[  101, 20704,  4609, 15431, 16480,  5339,  4244,   102,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int32)>},\n",
              "       {'input_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[  101, 20704,  4609, 15431,   102,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int32)>},\n",
              "       {'input_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[  101, 16480,  5339,  4244,   102,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int32)>}], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCFgGzL48gBp",
        "colab_type": "code",
        "outputId": "f64597ef-0ce5-4b7c-f4e1-94dc68bdb27c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "next(df.iterrows())[1]['Features']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>,\n",
              " 'input_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[  101,  1037,  2186,  1997,  9686, 17695, 18673, 14313,  1996,\n",
              "         15262,  3351,  2008,  2054,  2003,  2204,  2005,  1996, 13020,\n",
              "          2003,  2036,  2204,  2005,  1996, 25957,  4063,  1010,  2070,\n",
              "          1997,  2029,  5681,  2572, 25581,  2021,  3904,  1997,  2029,\n",
              "          8310,  2000,  2172,  1997,  1037,  2466,  1012,   102,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0]], dtype=int32)>,\n",
              " 'token_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6i_S37UEGgt",
        "colab_type": "code",
        "outputId": "600eea53-f40a-42ec-b822-d91758d3a2d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(df['Sentiment'].loc[0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYoLR846olJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_shape_list(tensor):\n",
        "  shape = tensor.shape.as_list()\n",
        "\n",
        "  non_static_indexes = []\n",
        "  for (index, dim) in enumerate(shape):\n",
        "    if dim is None:\n",
        "      non_static_indexes.append(index)\n",
        "\n",
        "  if not non_static_indexes:\n",
        "    return shape\n",
        "\n",
        "  dyn_shape = tf.shape(tensor)\n",
        "  for index in non_static_indexes:\n",
        "    shape[index] = dyn_shape[index]\n",
        "  return shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPSXc6yv1zQ8",
        "colab_type": "code",
        "outputId": "62cd8194-de26-4dbf-89aa-9c0106d02207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnP_5kYYD2gN",
        "colab_type": "code",
        "outputId": "def4b223-8152-4774-c06c-c671f36a216e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "  epochs = 3\n",
        "  batch_size = 1\n",
        "  for epoch in range(epochs):\n",
        "    for i, batch in df.iterrows():\n",
        "      with tf.GradientTape() as tape:\n",
        "        input_ids = batch['Features']['input_ids']\n",
        "        attention_mask = batch['Features']['attention_mask']\n",
        "        token_type_ids = batch['Features']['token_type_ids']\n",
        "        label_id = df['Sentiment'].loc[i]\n",
        "        outputs = model(input_ids, attention_mask= attention_mask, token_type_ids= token_type_ids, training= True)\n",
        "        logits = outputs[0]\n",
        "        logits_shape = get_shape_list(logits)[1]\n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "        labels = tf.one_hot(label_id, on_value=1.0, off_value=0.0, depth=logits_shape, dtype=tf.float32)\n",
        "        loss = -tf.reduce_mean(tf.reduce_sum(log_probs * labels, axis=-1))\n",
        "        grads = tape.gradient(loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "      if i % 200 == 0:\n",
        "        print('Training loss (for one batch) at step %s: %s' % (i, float(loss)))\n",
        "        print('Seen so far: %s samples' % ((i + 1) * batch_size))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss (for one batch) at step 0: 1.9212234020233154\n",
            "Seen so far: 1 samples\n",
            "Training loss (for one batch) at step 200: 1.7568427324295044\n",
            "Seen so far: 201 samples\n",
            "Training loss (for one batch) at step 400: 1.957686424255371\n",
            "Seen so far: 401 samples\n",
            "Training loss (for one batch) at step 600: 0.13277815282344818\n",
            "Seen so far: 601 samples\n",
            "Training loss (for one batch) at step 800: 2.304100751876831\n",
            "Seen so far: 801 samples\n",
            "Training loss (for one batch) at step 1000: 1.4647380113601685\n",
            "Seen so far: 1001 samples\n",
            "Training loss (for one batch) at step 1200: 0.5688464641571045\n",
            "Seen so far: 1201 samples\n",
            "Training loss (for one batch) at step 1400: 0.4697492718696594\n",
            "Seen so far: 1401 samples\n",
            "Training loss (for one batch) at step 1600: 0.5261104106903076\n",
            "Seen so far: 1601 samples\n",
            "Training loss (for one batch) at step 1800: 3.2130796909332275\n",
            "Seen so far: 1801 samples\n",
            "Training loss (for one batch) at step 2000: 0.3288925886154175\n",
            "Seen so far: 2001 samples\n",
            "Training loss (for one batch) at step 2200: 1.9885281324386597\n",
            "Seen so far: 2201 samples\n",
            "Training loss (for one batch) at step 2400: 1.5724353790283203\n",
            "Seen so far: 2401 samples\n",
            "Training loss (for one batch) at step 2600: 1.3242058753967285\n",
            "Seen so far: 2601 samples\n",
            "Training loss (for one batch) at step 2800: 0.4057513177394867\n",
            "Seen so far: 2801 samples\n",
            "Training loss (for one batch) at step 3000: 0.6375802755355835\n",
            "Seen so far: 3001 samples\n",
            "Training loss (for one batch) at step 3200: 0.960547685623169\n",
            "Seen so far: 3201 samples\n",
            "Training loss (for one batch) at step 3400: 0.752509355545044\n",
            "Seen so far: 3401 samples\n",
            "Training loss (for one batch) at step 3600: 0.5539364218711853\n",
            "Seen so far: 3601 samples\n",
            "Training loss (for one batch) at step 3800: 1.8588593006134033\n",
            "Seen so far: 3801 samples\n",
            "Training loss (for one batch) at step 4000: 0.4050409495830536\n",
            "Seen so far: 4001 samples\n",
            "Training loss (for one batch) at step 4200: 1.8949353694915771\n",
            "Seen so far: 4201 samples\n",
            "Training loss (for one batch) at step 4400: 1.661134958267212\n",
            "Seen so far: 4401 samples\n",
            "Training loss (for one batch) at step 4600: 1.8089371919631958\n",
            "Seen so far: 4601 samples\n",
            "Training loss (for one batch) at step 4800: 0.8585653305053711\n",
            "Seen so far: 4801 samples\n",
            "Training loss (for one batch) at step 5000: 1.516268014907837\n",
            "Seen so far: 5001 samples\n",
            "Training loss (for one batch) at step 5200: 0.7347781658172607\n",
            "Seen so far: 5201 samples\n",
            "Training loss (for one batch) at step 5400: 1.5487446784973145\n",
            "Seen so far: 5401 samples\n",
            "Training loss (for one batch) at step 5600: 1.1775882244110107\n",
            "Seen so far: 5601 samples\n",
            "Training loss (for one batch) at step 5800: 0.5053050518035889\n",
            "Seen so far: 5801 samples\n",
            "Training loss (for one batch) at step 6000: 0.7407081127166748\n",
            "Seen so far: 6001 samples\n",
            "Training loss (for one batch) at step 6200: 0.6947668790817261\n",
            "Seen so far: 6201 samples\n",
            "Training loss (for one batch) at step 6400: 0.636874258518219\n",
            "Seen so far: 6401 samples\n",
            "Training loss (for one batch) at step 6600: 2.120882987976074\n",
            "Seen so far: 6601 samples\n",
            "Training loss (for one batch) at step 6800: 0.7342883944511414\n",
            "Seen so far: 6801 samples\n",
            "Training loss (for one batch) at step 7000: 0.6375267505645752\n",
            "Seen so far: 7001 samples\n",
            "Training loss (for one batch) at step 7200: 1.3087303638458252\n",
            "Seen so far: 7201 samples\n",
            "Training loss (for one batch) at step 7400: 0.4628446698188782\n",
            "Seen so far: 7401 samples\n",
            "Training loss (for one batch) at step 7600: 0.6173148155212402\n",
            "Seen so far: 7601 samples\n",
            "Training loss (for one batch) at step 7800: 0.6081549525260925\n",
            "Seen so far: 7801 samples\n",
            "Training loss (for one batch) at step 8000: 0.6315214037895203\n",
            "Seen so far: 8001 samples\n",
            "Training loss (for one batch) at step 8200: 1.7103403806686401\n",
            "Seen so far: 8201 samples\n",
            "Training loss (for one batch) at step 8400: 0.4911198914051056\n",
            "Seen so far: 8401 samples\n",
            "Training loss (for one batch) at step 8600: 0.37301334738731384\n",
            "Seen so far: 8601 samples\n",
            "Training loss (for one batch) at step 8800: 1.7717570066452026\n",
            "Seen so far: 8801 samples\n",
            "Training loss (for one batch) at step 9000: 0.44033828377723694\n",
            "Seen so far: 9001 samples\n",
            "Training loss (for one batch) at step 9200: 1.2901736497879028\n",
            "Seen so far: 9201 samples\n",
            "Training loss (for one batch) at step 9400: 1.9984431266784668\n",
            "Seen so far: 9401 samples\n",
            "Training loss (for one batch) at step 9600: 1.651411771774292\n",
            "Seen so far: 9601 samples\n",
            "Training loss (for one batch) at step 9800: 0.7731359004974365\n",
            "Seen so far: 9801 samples\n",
            "Training loss (for one batch) at step 10000: 0.4095199406147003\n",
            "Seen so far: 10001 samples\n",
            "Training loss (for one batch) at step 10200: 0.9538654088973999\n",
            "Seen so far: 10201 samples\n",
            "Training loss (for one batch) at step 10400: 0.4597926139831543\n",
            "Seen so far: 10401 samples\n",
            "Training loss (for one batch) at step 10600: 0.35614335536956787\n",
            "Seen so far: 10601 samples\n",
            "Training loss (for one batch) at step 10800: 0.541036069393158\n",
            "Seen so far: 10801 samples\n",
            "Training loss (for one batch) at step 11000: 0.7395628094673157\n",
            "Seen so far: 11001 samples\n",
            "Training loss (for one batch) at step 11200: 0.5997443199157715\n",
            "Seen so far: 11201 samples\n",
            "Training loss (for one batch) at step 11400: 1.7615315914154053\n",
            "Seen so far: 11401 samples\n",
            "Training loss (for one batch) at step 11600: 0.5232172012329102\n",
            "Seen so far: 11601 samples\n",
            "Training loss (for one batch) at step 11800: 2.3601267337799072\n",
            "Seen so far: 11801 samples\n",
            "Training loss (for one batch) at step 12000: 1.5133607387542725\n",
            "Seen so far: 12001 samples\n",
            "Training loss (for one batch) at step 12200: 0.6667576432228088\n",
            "Seen so far: 12201 samples\n",
            "Training loss (for one batch) at step 12400: 0.8306121826171875\n",
            "Seen so far: 12401 samples\n",
            "Training loss (for one batch) at step 12600: 0.7291365265846252\n",
            "Seen so far: 12601 samples\n",
            "Training loss (for one batch) at step 12800: 0.7873163223266602\n",
            "Seen so far: 12801 samples\n",
            "Training loss (for one batch) at step 13000: 3.2045323848724365\n",
            "Seen so far: 13001 samples\n",
            "Training loss (for one batch) at step 13200: 0.9089550375938416\n",
            "Seen so far: 13201 samples\n",
            "Training loss (for one batch) at step 13400: 0.706412136554718\n",
            "Seen so far: 13401 samples\n",
            "Training loss (for one batch) at step 13600: 0.35133326053619385\n",
            "Seen so far: 13601 samples\n",
            "Training loss (for one batch) at step 13800: 0.9394785165786743\n",
            "Seen so far: 13801 samples\n",
            "Training loss (for one batch) at step 14000: 2.457152843475342\n",
            "Seen so far: 14001 samples\n",
            "Training loss (for one batch) at step 14200: 0.5644563436508179\n",
            "Seen so far: 14201 samples\n",
            "Training loss (for one batch) at step 14400: 0.5700392723083496\n",
            "Seen so far: 14401 samples\n",
            "Training loss (for one batch) at step 14600: 1.9934178590774536\n",
            "Seen so far: 14601 samples\n",
            "Training loss (for one batch) at step 14800: 0.657119631767273\n",
            "Seen so far: 14801 samples\n",
            "Training loss (for one batch) at step 15000: 0.6159687042236328\n",
            "Seen so far: 15001 samples\n",
            "Training loss (for one batch) at step 15200: 0.8976645469665527\n",
            "Seen so far: 15201 samples\n",
            "Training loss (for one batch) at step 15400: 3.762803792953491\n",
            "Seen so far: 15401 samples\n",
            "Training loss (for one batch) at step 15600: 0.29342207312583923\n",
            "Seen so far: 15601 samples\n",
            "Training loss (for one batch) at step 15800: 2.374972105026245\n",
            "Seen so far: 15801 samples\n",
            "Training loss (for one batch) at step 16000: 0.6158746480941772\n",
            "Seen so far: 16001 samples\n",
            "Training loss (for one batch) at step 16200: 2.6315596103668213\n",
            "Seen so far: 16201 samples\n",
            "Training loss (for one batch) at step 16400: 0.4377160668373108\n",
            "Seen so far: 16401 samples\n",
            "Training loss (for one batch) at step 16600: 0.6838018298149109\n",
            "Seen so far: 16601 samples\n",
            "Training loss (for one batch) at step 16800: 2.0284383296966553\n",
            "Seen so far: 16801 samples\n",
            "Training loss (for one batch) at step 17000: 0.5214588046073914\n",
            "Seen so far: 17001 samples\n",
            "Training loss (for one batch) at step 17200: 2.536674737930298\n",
            "Seen so far: 17201 samples\n",
            "Training loss (for one batch) at step 17400: 2.396667003631592\n",
            "Seen so far: 17401 samples\n",
            "Training loss (for one batch) at step 17600: 1.099867820739746\n",
            "Seen so far: 17601 samples\n",
            "Training loss (for one batch) at step 17800: 1.7549256086349487\n",
            "Seen so far: 17801 samples\n",
            "Training loss (for one batch) at step 18000: 1.9349392652511597\n",
            "Seen so far: 18001 samples\n",
            "Training loss (for one batch) at step 18200: 0.4479339122772217\n",
            "Seen so far: 18201 samples\n",
            "Training loss (for one batch) at step 18400: 0.9531606435775757\n",
            "Seen so far: 18401 samples\n",
            "Training loss (for one batch) at step 18600: 0.40250059962272644\n",
            "Seen so far: 18601 samples\n",
            "Training loss (for one batch) at step 18800: 3.3479344844818115\n",
            "Seen so far: 18801 samples\n",
            "Training loss (for one batch) at step 19000: 0.3411066234111786\n",
            "Seen so far: 19001 samples\n",
            "Training loss (for one batch) at step 19200: 0.990204393863678\n",
            "Seen so far: 19201 samples\n",
            "Training loss (for one batch) at step 19400: 0.4858613610267639\n",
            "Seen so far: 19401 samples\n",
            "Training loss (for one batch) at step 19600: 1.7899894714355469\n",
            "Seen so far: 19601 samples\n",
            "Training loss (for one batch) at step 19800: 0.47828036546707153\n",
            "Seen so far: 19801 samples\n",
            "Training loss (for one batch) at step 20000: 2.2536582946777344\n",
            "Seen so far: 20001 samples\n",
            "Training loss (for one batch) at step 20200: 1.376091480255127\n",
            "Seen so far: 20201 samples\n",
            "Training loss (for one batch) at step 20400: 0.5918846726417542\n",
            "Seen so far: 20401 samples\n",
            "Training loss (for one batch) at step 20600: 0.534856915473938\n",
            "Seen so far: 20601 samples\n",
            "Training loss (for one batch) at step 20800: 0.4774496853351593\n",
            "Seen so far: 20801 samples\n",
            "Training loss (for one batch) at step 21000: 0.458617240190506\n",
            "Seen so far: 21001 samples\n",
            "Training loss (for one batch) at step 21200: 0.7115517258644104\n",
            "Seen so far: 21201 samples\n",
            "Training loss (for one batch) at step 21400: 0.6652278304100037\n",
            "Seen so far: 21401 samples\n",
            "Training loss (for one batch) at step 21600: 2.297948122024536\n",
            "Seen so far: 21601 samples\n",
            "Training loss (for one batch) at step 21800: 1.512962818145752\n",
            "Seen so far: 21801 samples\n",
            "Training loss (for one batch) at step 22000: 0.6572699546813965\n",
            "Seen so far: 22001 samples\n",
            "Training loss (for one batch) at step 22200: 4.144139289855957\n",
            "Seen so far: 22201 samples\n",
            "Training loss (for one batch) at step 22400: 0.6965065598487854\n",
            "Seen so far: 22401 samples\n",
            "Training loss (for one batch) at step 22600: 2.4148643016815186\n",
            "Seen so far: 22601 samples\n",
            "Training loss (for one batch) at step 22800: 1.0045828819274902\n",
            "Seen so far: 22801 samples\n",
            "Training loss (for one batch) at step 23000: 1.70160973072052\n",
            "Seen so far: 23001 samples\n",
            "Training loss (for one batch) at step 23200: 0.513433575630188\n",
            "Seen so far: 23201 samples\n",
            "Training loss (for one batch) at step 23400: 2.4517409801483154\n",
            "Seen so far: 23401 samples\n",
            "Training loss (for one batch) at step 23600: 0.6651251912117004\n",
            "Seen so far: 23601 samples\n",
            "Training loss (for one batch) at step 23800: 0.4402322769165039\n",
            "Seen so far: 23801 samples\n",
            "Training loss (for one batch) at step 24000: 1.4270875453948975\n",
            "Seen so far: 24001 samples\n",
            "Training loss (for one batch) at step 24200: 1.7031123638153076\n",
            "Seen so far: 24201 samples\n",
            "Training loss (for one batch) at step 24400: 3.883329391479492\n",
            "Seen so far: 24401 samples\n",
            "Training loss (for one batch) at step 24600: 1.6987652778625488\n",
            "Seen so far: 24601 samples\n",
            "Training loss (for one batch) at step 24800: 2.3915622234344482\n",
            "Seen so far: 24801 samples\n",
            "Training loss (for one batch) at step 25000: 1.9086641073226929\n",
            "Seen so far: 25001 samples\n",
            "Training loss (for one batch) at step 25200: 0.5100736021995544\n",
            "Seen so far: 25201 samples\n",
            "Training loss (for one batch) at step 25400: 1.3924546241760254\n",
            "Seen so far: 25401 samples\n",
            "Training loss (for one batch) at step 25600: 0.5282192230224609\n",
            "Seen so far: 25601 samples\n",
            "Training loss (for one batch) at step 25800: 1.3912005424499512\n",
            "Seen so far: 25801 samples\n",
            "Training loss (for one batch) at step 26000: 4.105310440063477\n",
            "Seen so far: 26001 samples\n",
            "Training loss (for one batch) at step 26200: 0.5674991607666016\n",
            "Seen so far: 26201 samples\n",
            "Training loss (for one batch) at step 26400: 1.1614437103271484\n",
            "Seen so far: 26401 samples\n",
            "Training loss (for one batch) at step 26600: 0.5754718780517578\n",
            "Seen so far: 26601 samples\n",
            "Training loss (for one batch) at step 26800: 0.5389796495437622\n",
            "Seen so far: 26801 samples\n",
            "Training loss (for one batch) at step 27000: 0.5931708812713623\n",
            "Seen so far: 27001 samples\n",
            "Training loss (for one batch) at step 27200: 0.78419429063797\n",
            "Seen so far: 27201 samples\n",
            "Training loss (for one batch) at step 27400: 0.5576593279838562\n",
            "Seen so far: 27401 samples\n",
            "Training loss (for one batch) at step 27600: 1.6779758930206299\n",
            "Seen so far: 27601 samples\n",
            "Training loss (for one batch) at step 27800: 0.58847576379776\n",
            "Seen so far: 27801 samples\n",
            "Training loss (for one batch) at step 28000: 1.3473832607269287\n",
            "Seen so far: 28001 samples\n",
            "Training loss (for one batch) at step 28200: 1.2502385377883911\n",
            "Seen so far: 28201 samples\n",
            "Training loss (for one batch) at step 28400: 0.7562245726585388\n",
            "Seen so far: 28401 samples\n",
            "Training loss (for one batch) at step 28600: 2.9076082706451416\n",
            "Seen so far: 28601 samples\n",
            "Training loss (for one batch) at step 28800: 1.1541205644607544\n",
            "Seen so far: 28801 samples\n",
            "Training loss (for one batch) at step 29000: 0.3848346471786499\n",
            "Seen so far: 29001 samples\n",
            "Training loss (for one batch) at step 29200: 1.6921093463897705\n",
            "Seen so far: 29201 samples\n",
            "Training loss (for one batch) at step 29400: 1.4458328485488892\n",
            "Seen so far: 29401 samples\n",
            "Training loss (for one batch) at step 29600: 1.5328857898712158\n",
            "Seen so far: 29601 samples\n",
            "Training loss (for one batch) at step 29800: 2.0161848068237305\n",
            "Seen so far: 29801 samples\n",
            "Training loss (for one batch) at step 30000: 2.1352672576904297\n",
            "Seen so far: 30001 samples\n",
            "Training loss (for one batch) at step 30200: 2.768617630004883\n",
            "Seen so far: 30201 samples\n",
            "Training loss (for one batch) at step 30400: 0.7120435237884521\n",
            "Seen so far: 30401 samples\n",
            "Training loss (for one batch) at step 30600: 3.451286554336548\n",
            "Seen so far: 30601 samples\n",
            "Training loss (for one batch) at step 30800: 0.36286816000938416\n",
            "Seen so far: 30801 samples\n",
            "Training loss (for one batch) at step 31000: 0.5630350112915039\n",
            "Seen so far: 31001 samples\n",
            "Training loss (for one batch) at step 31200: 1.2108317613601685\n",
            "Seen so far: 31201 samples\n",
            "Training loss (for one batch) at step 31400: 3.43156099319458\n",
            "Seen so far: 31401 samples\n",
            "Training loss (for one batch) at step 31600: 1.5883338451385498\n",
            "Seen so far: 31601 samples\n",
            "Training loss (for one batch) at step 31800: 1.2713490724563599\n",
            "Seen so far: 31801 samples\n",
            "Training loss (for one batch) at step 32000: 2.825732469558716\n",
            "Seen so far: 32001 samples\n",
            "Training loss (for one batch) at step 32200: 1.427213191986084\n",
            "Seen so far: 32201 samples\n",
            "Training loss (for one batch) at step 32400: 0.6524211168289185\n",
            "Seen so far: 32401 samples\n",
            "Training loss (for one batch) at step 32600: 1.034788727760315\n",
            "Seen so far: 32601 samples\n",
            "Training loss (for one batch) at step 32800: 0.8553857207298279\n",
            "Seen so far: 32801 samples\n",
            "Training loss (for one batch) at step 33000: 1.6676450967788696\n",
            "Seen so far: 33001 samples\n",
            "Training loss (for one batch) at step 33200: 1.6858155727386475\n",
            "Seen so far: 33201 samples\n",
            "Training loss (for one batch) at step 33400: 0.7310950756072998\n",
            "Seen so far: 33401 samples\n",
            "Training loss (for one batch) at step 33600: 0.9158506393432617\n",
            "Seen so far: 33601 samples\n",
            "Training loss (for one batch) at step 33800: 0.47801917791366577\n",
            "Seen so far: 33801 samples\n",
            "Training loss (for one batch) at step 34000: 3.0431768894195557\n",
            "Seen so far: 34001 samples\n",
            "Training loss (for one batch) at step 34200: 1.002480149269104\n",
            "Seen so far: 34201 samples\n",
            "Training loss (for one batch) at step 34400: 0.39286142587661743\n",
            "Seen so far: 34401 samples\n",
            "Training loss (for one batch) at step 34600: 1.7707163095474243\n",
            "Seen so far: 34601 samples\n",
            "Training loss (for one batch) at step 34800: 2.9564619064331055\n",
            "Seen so far: 34801 samples\n",
            "Training loss (for one batch) at step 35000: 0.8038689494132996\n",
            "Seen so far: 35001 samples\n",
            "Training loss (for one batch) at step 35200: 1.7553988695144653\n",
            "Seen so far: 35201 samples\n",
            "Training loss (for one batch) at step 35400: 2.925638437271118\n",
            "Seen so far: 35401 samples\n",
            "Training loss (for one batch) at step 35600: 1.501574158668518\n",
            "Seen so far: 35601 samples\n",
            "Training loss (for one batch) at step 35800: 0.820298433303833\n",
            "Seen so far: 35801 samples\n",
            "Training loss (for one batch) at step 36000: 0.6033400893211365\n",
            "Seen so far: 36001 samples\n",
            "Training loss (for one batch) at step 36200: 1.880055546760559\n",
            "Seen so far: 36201 samples\n",
            "Training loss (for one batch) at step 36400: 2.025986909866333\n",
            "Seen so far: 36401 samples\n",
            "Training loss (for one batch) at step 36600: 4.033719062805176\n",
            "Seen so far: 36601 samples\n",
            "Training loss (for one batch) at step 36800: 0.7221643924713135\n",
            "Seen so far: 36801 samples\n",
            "Training loss (for one batch) at step 37000: 1.8747390508651733\n",
            "Seen so far: 37001 samples\n",
            "Training loss (for one batch) at step 37200: 1.1297858953475952\n",
            "Seen so far: 37201 samples\n",
            "Training loss (for one batch) at step 37400: 0.8974820375442505\n",
            "Seen so far: 37401 samples\n",
            "Training loss (for one batch) at step 37600: 1.7283985614776611\n",
            "Seen so far: 37601 samples\n",
            "Training loss (for one batch) at step 37800: 2.7390451431274414\n",
            "Seen so far: 37801 samples\n",
            "Training loss (for one batch) at step 38000: 1.691166877746582\n",
            "Seen so far: 38001 samples\n",
            "Training loss (for one batch) at step 38200: 1.3984644412994385\n",
            "Seen so far: 38201 samples\n",
            "Training loss (for one batch) at step 38400: 1.4579778909683228\n",
            "Seen so far: 38401 samples\n",
            "Training loss (for one batch) at step 38600: 0.43372461199760437\n",
            "Seen so far: 38601 samples\n",
            "Training loss (for one batch) at step 38800: 0.7019580006599426\n",
            "Seen so far: 38801 samples\n",
            "Training loss (for one batch) at step 39000: 0.46279245615005493\n",
            "Seen so far: 39001 samples\n",
            "Training loss (for one batch) at step 39200: 0.8998461365699768\n",
            "Seen so far: 39201 samples\n",
            "Training loss (for one batch) at step 39400: 2.9881575107574463\n",
            "Seen so far: 39401 samples\n",
            "Training loss (for one batch) at step 39600: 2.370096206665039\n",
            "Seen so far: 39601 samples\n",
            "Training loss (for one batch) at step 39800: 0.665859580039978\n",
            "Seen so far: 39801 samples\n",
            "Training loss (for one batch) at step 40000: 0.6734793782234192\n",
            "Seen so far: 40001 samples\n",
            "Training loss (for one batch) at step 40200: 1.9041056632995605\n",
            "Seen so far: 40201 samples\n",
            "Training loss (for one batch) at step 40400: 1.5460021495819092\n",
            "Seen so far: 40401 samples\n",
            "Training loss (for one batch) at step 40600: 1.1924561262130737\n",
            "Seen so far: 40601 samples\n",
            "Training loss (for one batch) at step 40800: 1.772233486175537\n",
            "Seen so far: 40801 samples\n",
            "Training loss (for one batch) at step 41000: 0.6405764818191528\n",
            "Seen so far: 41001 samples\n",
            "Training loss (for one batch) at step 41200: 1.896845817565918\n",
            "Seen so far: 41201 samples\n",
            "Training loss (for one batch) at step 41400: 1.7965737581253052\n",
            "Seen so far: 41401 samples\n",
            "Training loss (for one batch) at step 41600: 0.7323064208030701\n",
            "Seen so far: 41601 samples\n",
            "Training loss (for one batch) at step 41800: 0.3915298581123352\n",
            "Seen so far: 41801 samples\n",
            "Training loss (for one batch) at step 42000: 0.6477649807929993\n",
            "Seen so far: 42001 samples\n",
            "Training loss (for one batch) at step 42200: 1.9793496131896973\n",
            "Seen so far: 42201 samples\n",
            "Training loss (for one batch) at step 42400: 1.3328713178634644\n",
            "Seen so far: 42401 samples\n",
            "Training loss (for one batch) at step 42600: 1.8882644176483154\n",
            "Seen so far: 42601 samples\n",
            "Training loss (for one batch) at step 42800: 1.182982325553894\n",
            "Seen so far: 42801 samples\n",
            "Training loss (for one batch) at step 43000: 1.9268062114715576\n",
            "Seen so far: 43001 samples\n",
            "Training loss (for one batch) at step 43200: 0.49402856826782227\n",
            "Seen so far: 43201 samples\n",
            "Training loss (for one batch) at step 43400: 0.493263304233551\n",
            "Seen so far: 43401 samples\n",
            "Training loss (for one batch) at step 43600: 0.6141701340675354\n",
            "Seen so far: 43601 samples\n",
            "Training loss (for one batch) at step 43800: 1.3918439149856567\n",
            "Seen so far: 43801 samples\n",
            "Training loss (for one batch) at step 44000: 2.7186927795410156\n",
            "Seen so far: 44001 samples\n",
            "Training loss (for one batch) at step 44200: 1.500791072845459\n",
            "Seen so far: 44201 samples\n",
            "Training loss (for one batch) at step 44400: 0.8011974096298218\n",
            "Seen so far: 44401 samples\n",
            "Training loss (for one batch) at step 44600: 3.8089866638183594\n",
            "Seen so far: 44601 samples\n",
            "Training loss (for one batch) at step 44800: 0.5495873689651489\n",
            "Seen so far: 44801 samples\n",
            "Training loss (for one batch) at step 45000: 0.3642386794090271\n",
            "Seen so far: 45001 samples\n",
            "Training loss (for one batch) at step 45200: 1.0877102613449097\n",
            "Seen so far: 45201 samples\n",
            "Training loss (for one batch) at step 45400: 1.093626618385315\n",
            "Seen so far: 45401 samples\n",
            "Training loss (for one batch) at step 45600: 0.6518852114677429\n",
            "Seen so far: 45601 samples\n",
            "Training loss (for one batch) at step 45800: 1.4520864486694336\n",
            "Seen so far: 45801 samples\n",
            "Training loss (for one batch) at step 46000: 2.6275014877319336\n",
            "Seen so far: 46001 samples\n",
            "Training loss (for one batch) at step 46200: 0.6261141300201416\n",
            "Seen so far: 46201 samples\n",
            "Training loss (for one batch) at step 46400: 2.389967441558838\n",
            "Seen so far: 46401 samples\n",
            "Training loss (for one batch) at step 46600: 1.4966299533843994\n",
            "Seen so far: 46601 samples\n",
            "Training loss (for one batch) at step 46800: 1.8518664836883545\n",
            "Seen so far: 46801 samples\n",
            "Training loss (for one batch) at step 47000: 3.3329660892486572\n",
            "Seen so far: 47001 samples\n",
            "Training loss (for one batch) at step 47200: 1.9235864877700806\n",
            "Seen so far: 47201 samples\n",
            "Training loss (for one batch) at step 47400: 2.2682814598083496\n",
            "Seen so far: 47401 samples\n",
            "Training loss (for one batch) at step 47600: 0.813281774520874\n",
            "Seen so far: 47601 samples\n",
            "Training loss (for one batch) at step 47800: 1.6674847602844238\n",
            "Seen so far: 47801 samples\n",
            "Training loss (for one batch) at step 48000: 1.9775593280792236\n",
            "Seen so far: 48001 samples\n",
            "Training loss (for one batch) at step 48200: 0.46099936962127686\n",
            "Seen so far: 48201 samples\n",
            "Training loss (for one batch) at step 48400: 0.9666993618011475\n",
            "Seen so far: 48401 samples\n",
            "Training loss (for one batch) at step 48600: 2.0248637199401855\n",
            "Seen so far: 48601 samples\n",
            "Training loss (for one batch) at step 48800: 2.825578212738037\n",
            "Seen so far: 48801 samples\n",
            "Training loss (for one batch) at step 49000: 0.7483512759208679\n",
            "Seen so far: 49001 samples\n",
            "Training loss (for one batch) at step 49200: 0.44677525758743286\n",
            "Seen so far: 49201 samples\n",
            "Training loss (for one batch) at step 49400: 2.3168182373046875\n",
            "Seen so far: 49401 samples\n",
            "Training loss (for one batch) at step 49600: 0.5983847975730896\n",
            "Seen so far: 49601 samples\n",
            "Training loss (for one batch) at step 49800: 3.1453685760498047\n",
            "Seen so far: 49801 samples\n",
            "Training loss (for one batch) at step 50000: 3.053856611251831\n",
            "Seen so far: 50001 samples\n",
            "Training loss (for one batch) at step 50200: 1.197817325592041\n",
            "Seen so far: 50201 samples\n",
            "Training loss (for one batch) at step 50400: 2.6628026962280273\n",
            "Seen so far: 50401 samples\n",
            "Training loss (for one batch) at step 50600: 2.109816074371338\n",
            "Seen so far: 50601 samples\n",
            "Training loss (for one batch) at step 50800: 1.725604772567749\n",
            "Seen so far: 50801 samples\n",
            "Training loss (for one batch) at step 51000: 0.9149419069290161\n",
            "Seen so far: 51001 samples\n",
            "Training loss (for one batch) at step 51200: 0.6030592918395996\n",
            "Seen so far: 51201 samples\n",
            "Training loss (for one batch) at step 51400: 0.4895913004875183\n",
            "Seen so far: 51401 samples\n",
            "Training loss (for one batch) at step 51600: 1.5446257591247559\n",
            "Seen so far: 51601 samples\n",
            "Training loss (for one batch) at step 51800: 2.0294456481933594\n",
            "Seen so far: 51801 samples\n",
            "Training loss (for one batch) at step 52000: 0.7801331281661987\n",
            "Seen so far: 52001 samples\n",
            "Training loss (for one batch) at step 52200: 1.2757123708724976\n",
            "Seen so far: 52201 samples\n",
            "Training loss (for one batch) at step 52400: 1.4651694297790527\n",
            "Seen so far: 52401 samples\n",
            "Training loss (for one batch) at step 52600: 2.760744094848633\n",
            "Seen so far: 52601 samples\n",
            "Training loss (for one batch) at step 52800: 1.5668466091156006\n",
            "Seen so far: 52801 samples\n",
            "Training loss (for one batch) at step 53000: 0.5340741872787476\n",
            "Seen so far: 53001 samples\n",
            "Training loss (for one batch) at step 53200: 1.5488941669464111\n",
            "Seen so far: 53201 samples\n",
            "Training loss (for one batch) at step 53400: 0.9038934707641602\n",
            "Seen so far: 53401 samples\n",
            "Training loss (for one batch) at step 53600: 1.7389439344406128\n",
            "Seen so far: 53601 samples\n",
            "Training loss (for one batch) at step 53800: 1.1115493774414062\n",
            "Seen so far: 53801 samples\n",
            "Training loss (for one batch) at step 54000: 0.8692507147789001\n",
            "Seen so far: 54001 samples\n",
            "Training loss (for one batch) at step 54200: 2.019153594970703\n",
            "Seen so far: 54201 samples\n",
            "Training loss (for one batch) at step 54400: 0.9665783643722534\n",
            "Seen so far: 54401 samples\n",
            "Training loss (for one batch) at step 54600: 2.021181344985962\n",
            "Seen so far: 54601 samples\n",
            "Training loss (for one batch) at step 54800: 0.8831180334091187\n",
            "Seen so far: 54801 samples\n",
            "Training loss (for one batch) at step 55000: 1.4933598041534424\n",
            "Seen so far: 55001 samples\n",
            "Training loss (for one batch) at step 55200: 1.7966445684432983\n",
            "Seen so far: 55201 samples\n",
            "Training loss (for one batch) at step 55400: 0.4415660500526428\n",
            "Seen so far: 55401 samples\n",
            "Training loss (for one batch) at step 55600: 0.8527066707611084\n",
            "Seen so far: 55601 samples\n",
            "Training loss (for one batch) at step 55800: 0.666065514087677\n",
            "Seen so far: 55801 samples\n",
            "Training loss (for one batch) at step 56000: 2.0134811401367188\n",
            "Seen so far: 56001 samples\n",
            "Training loss (for one batch) at step 56200: 0.7368241548538208\n",
            "Seen so far: 56201 samples\n",
            "Training loss (for one batch) at step 56400: 3.1153314113616943\n",
            "Seen so far: 56401 samples\n",
            "Training loss (for one batch) at step 56600: 2.1181085109710693\n",
            "Seen so far: 56601 samples\n",
            "Training loss (for one batch) at step 56800: 0.62633216381073\n",
            "Seen so far: 56801 samples\n",
            "Training loss (for one batch) at step 57000: 0.8621433973312378\n",
            "Seen so far: 57001 samples\n",
            "Training loss (for one batch) at step 57200: 0.666537880897522\n",
            "Seen so far: 57201 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWycE96KCbXI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "82e8a555-ace1-4c3d-f4c9-83b9046d3aef"
      },
      "source": [
        "model.save_pretrained('./save_model/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-76139296d809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./save_model/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_15EJXOF-y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = pd.read_csv(\"test.tsv\", sep='\\t', engine='python')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5W1qox5LgyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test['Features'] = df_test['Phrase'].apply(convert_examples_to_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-c6a3HloYrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = []\n",
        "for i, batch in df_test.iterrows():\n",
        "  input_ids = batch['Features']['input_ids']\n",
        "  attention_mask = batch['Features']['attention_mask']\n",
        "  token_type_ids = batch['Features']['token_type_ids']\n",
        "  outputs = model(input_ids, attention_mask= attention_mask, token_type_ids= token_type_ids, training= True)\n",
        "  logits = outputs[0]\n",
        "  label_id = tf.argmax(logits.numpy()[0]).numpy()\n",
        "  labels.append(label_id)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}